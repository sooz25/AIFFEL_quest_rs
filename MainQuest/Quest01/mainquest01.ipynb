{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e4163d-9642-4fe6-b4f3-6ea8e6b79904",
   "metadata": {},
   "source": [
    "[Transformer vs GPT ë¹„êµ]\n",
    "- ì¼ë°˜ì ì¸ TransformerëŠ” ì¸ì½”ë” ë° ë””ì½”ë”ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ë§Œ, gptëŠ” ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "- í•™ìŠµëª©í‘œëŠ” ë¹„ì§€ë„ ì–¸ì–´ëª¨ë¸(ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ) ëª©ì  ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ë‘ê³ , íƒœìŠ¤í¬ë³„ íŒŒì¸íŠœë‹ ì‹œ ì‘ì€ ì„ í˜• í—¤ë“œ(W_y)ë§Œ ì¶”ê°€í•œë‹¤.\n",
    "- masked self attentionìœ¼ë¡œ ë¯¸ë˜ í† í°ì„ ì°¨ë‹¨í•œë‹¤.\n",
    "- ì…ë ¥ë³€í™˜(traversal-style)ìœ¼ë¡œ êµ¬ì¡°í™” ì…ë ¥(ë¬¸ì¥ìŒ, ë¬¸ë§¥+ë‹µ ë“±)ì„ í•˜ë‚˜ì˜ ì—°ì† ì‹œí€€ìŠ¤ë¡œ ì¬êµ¬ì„±í•œë‹¤.\n",
    "- ì¶”ê°€ íŒŒë¼ë¯¸í„°ëŠ” í—¤ë“œ W_yì™€ íŠ¹ìˆ˜ í† í° ì„ë² ë”©(ã€ˆsã€‰, ã€ˆeã€‰, ã€ˆdelimã€‰)ë¿ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8793f38d-4d20-4b9a-90c5-88484a543c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588587df-cb2b-497f-a3ac-e717731fab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-05 07:38:12--  https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\n",
      "Resolving github.com (github.com)... 20.27.177.113\n",
      "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv [following]\n",
      "--2025-11-05 07:38:13--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 889842 (869K) [text/plain]\n",
      "Saving to: â€˜ChatbotData.csv.1â€™\n",
      "\n",
      "ChatbotData.csv.1   100%[===================>] 868.99K  4.92MB/s    in 0.2s    \n",
      "\n",
      "2025-11-05 07:38:13 (4.92 MB/s) - â€˜ChatbotData.csv.1â€™ saved [889842/889842]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p ~/work/transformer_chatbot/data/ && cd ~/work/transformer_chatbot/data/\n",
    "! wget https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a80976-48fd-43fe-920c-44fd5f19c800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118 True\n"
     ]
    }
   ],
   "source": [
    "# í´ë”/íŒ¨í‚¤ì§€ ì¤€ë¹„\n",
    "!pip -q install transformers==4.43.3 accelerate sentencepiece datasets\n",
    "import os, pandas as pd, torch, numpy as np\n",
    "print(torch.__version__, torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af7b23f3-ece2-4faa-9ebe-b2dfbd397a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11740,\n",
       "               Q            A\n",
       " 0        12ì‹œ ë•¡!   í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.\n",
       " 1   1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´    ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.\n",
       " 2  3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° í™•ì¸ ë° ì •ë¦¬\n",
    "\n",
    "import pandas as pd, re\n",
    "path = os.path.expanduser('~/work/transformer_chatbot/data/ChatbotData.csv')\n",
    "df = pd.read_csv(path)\n",
    "df = df.rename(columns={df.columns[0]:'Q', df.columns[1]:'A'})  # ì•ˆì „í•˜ê²Œ\n",
    "df = df[['Q','A']].dropna().drop_duplicates()\n",
    "\n",
    "# ê°„ë‹¨ í´ë¦°ì—…(ì´ëª¨ì§€/ì´ìƒë¬¸ì ì œê±°Â·ì—°ì† ê³µë°± ì¶•ì†Œ)\n",
    "def clean(s):\n",
    "    s = re.sub(r'[^\\S\\r\\n]+', ' ', str(s)).strip()\n",
    "    s = re.sub(r'[\\u200b-\\u200f]', '', s)\n",
    "    return s\n",
    "df['Q'] = df['Q'].map(clean)\n",
    "df['A'] = df['A'].map(clean)\n",
    "\n",
    "# ê¸¸ì´ í•„í„°(ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸´ ìŒ ì œê±°)\n",
    "df = df[(df['Q'].str.len().between(2, 100)) & (df['A'].str.len().between(2, 120))]\n",
    "len(df), df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c59d81-f638-45dc-b36d-f35dd4f0b786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11153, 587)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•™ìŠµ/ ê²€ì¦ ë¶„ë¦¬\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(df, test_size=0.05, random_state=42, shuffle=True)\n",
    "len(train_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc09a7-8c2f-46b0-aad6-bfcec282411e",
   "metadata": {},
   "source": [
    "[í† í¬ë‚˜ì´ì €/KoGPT2 ë¡œë“œ]\n",
    "- ì‚¬ì „í•™ìŠµëœ KoGPR2ë¥¼ ë°±ë³¸ìœ¼ë¡œ ì‚¬ìš©í•´ GPTì˜ Decoder-only êµ¬ì¡°ë¥¼ ì‹¤ì œ í™˜ê²½ì—ì„œ ì¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39554ba-c243-43ab-a9ec-908bcf8e8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) í† í¬ë‚˜ì´ì €/ëª¨ë¸ (KoGPT2)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "SEP = \"<unused0>\"   # KoGPT2ì— ë‚¨ëŠ” unused í† í° í•˜ë‚˜ë¥¼ êµ¬ë¶„ìë¡œ ì‚¬ìš©\n",
    "bos, eos, pad = tok.bos_token, tok.eos_token, tok.pad_token or \"<pad>\"\n",
    "\n",
    "if tok.pad_token is None:\n",
    "    tok.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "    model.resize_token_embeddings(len(tok))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724e19f-4091-49b8-9410-be32aa79587d",
   "metadata": {},
   "source": [
    "[íŠ¹ìˆ˜ í† í° ì¶”ê°€ + pack ì…ë ¥ ë³€í™˜](2ë²ˆ)\n",
    "- ë¬¸ì¥ìŒê³¼ ê°ê´€ì‹ ì…ë ¥ì„ ì—°ì† ì‹œí€€ìŠ¤ë¡œ ì§ë ¬í™”í•´ ì‚¬ì „í•™ìŠµëœ ì–¸ì–´ëª¨ë¸ì´ ê·¸ëŒ€ë¡œ ì²˜ë¦¬í•˜ë„ë¡ í–ˆìœ¼ë©°, ì´ëŠ” GPT ë…¼ë¬¸ì˜ traversal-style ì…ë ¥ê³¼ ë™ì¼í•œ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74699241-15f9-494b-b536-5db0f72d6d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: 256 256 256 256 256\n"
     ]
    }
   ],
   "source": [
    "# === íŠ¹ìˆ˜ í† í° ì¶”ê°€ ===\n",
    "SPECIALS = {\"additional_special_tokens\": [\"<s>\", \"</s>\", \"<delim>\", \"<extract>\"]}\n",
    "tok.add_special_tokens(SPECIALS)\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "TOK_S     = tok.convert_tokens_to_ids(\"<s>\")\n",
    "TOK_E     = tok.convert_tokens_to_ids(\"</s>\")\n",
    "TOK_DELIM = tok.convert_tokens_to_ids(\"<delim>\")\n",
    "TOK_EXT   = tok.convert_tokens_to_ids(\"<extract>\")\n",
    "\n",
    "MAX_LEN = 256\n",
    "\n",
    "def pad_to_len(ids, L=MAX_LEN, pad_id=tok.pad_token_id):\n",
    "    return (ids + [pad_id]*(L-len(ids)))[:L]\n",
    "\n",
    "# --- Fig.1 í¬ë§· ---\n",
    "def pack_classification(text):\n",
    "    ids = tok.encode(\"<s> \" + text + \" </s>\", add_special_tokens=False);  return pad_to_len(ids)\n",
    "\n",
    "def pack_entailment(premise, hypothesis):\n",
    "    ids = tok.encode(\"<s> \" + premise + \" <delim> \" + hypothesis + \" </s>\", add_special_tokens=False);  return pad_to_len(ids)\n",
    "\n",
    "def pack_similarity(a, b):\n",
    "    ab = tok.encode(\"<s> \" + a + \" <delim> \" + b + \" </s>\", add_special_tokens=False)\n",
    "    ba = tok.encode(\"<s> \" + b + \" <delim> \" + a + \" </s>\", add_special_tokens=False)\n",
    "    return pad_to_len(ab), pad_to_len(ba)\n",
    "\n",
    "def pack_mc(context, answer):\n",
    "    ids = tok.encode(\"<s> \" + context + \" <delim> \" + answer + \" </s>\", add_special_tokens=False);  return pad_to_len(ids)\n",
    "\n",
    "# ë¬´ê²°ì„± ì²´í¬\n",
    "ex_cls = pack_classification(\"ì´ ëª¨ë¸ì€ ì…ë ¥ ë³€í™˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "ex_ent = pack_entailment(\"ë¹„ëŠ” ë‚´ë¦°ë‹¤.\", \"ìš°ì‚°ì´ í•„ìš”í•˜ë‹¤.\")\n",
    "ex_sim = pack_similarity(\"ë‚˜ëŠ” ì¶•êµ¬ë¥¼ ì¢‹ì•„í•´\", \"ë‚˜ëŠ” í’‹ë³¼ì„ ì¢‹ì•„í•´\")\n",
    "ex_mc  = pack_mc(\"ì§€êµ¬ëŠ” ë¬´ì—‡ì˜ ì£¼ìœ„ë¥¼ ë„ëŠ”ê°€?\", \"íƒœì–‘\")\n",
    "print(\"OK:\", len(ex_cls), len(ex_ent), len(ex_sim[0]), len(ex_sim[1]), len(ex_mc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355ef62e-964d-441d-bd0f-255fe45f2565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì€ë‹‰ ë²¡í„° shape: (6, 768)\n",
      "attention_mask(1í–‰ ì• 32): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "position_ids(1í–‰ ì• 32): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.config.output_hidden_states = True  # ì€ë‹‰ì¸µ ì¶œë ¥\n",
    "\n",
    "def make_inputs(ids_list):\n",
    "    input_ids = torch.tensor(ids_list, dtype=torch.long)\n",
    "    attention_mask = (input_ids != tok.pad_token_id).long()\n",
    "    # íŒ¨ë”© ì œì™¸ 0..L-1 ìœ„ì¹˜ ë¶€ì—¬\n",
    "    position_ids = attention_mask.cumsum(dim=1) - 1\n",
    "    position_ids = position_ids.clamp(min=0)\n",
    "    return input_ids.to(device), attention_mask.to(device), position_ids.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def last_hidden_vec(input_ids, attention_mask, position_ids=None):\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, output_hidden_states=True)\n",
    "    h = out.hidden_states[-1]                  # [B, L, H]\n",
    "    last_idx = attention_mask.sum(dim=1) - 1   # ì‹¤ì œ ë§ˆì§€ë§‰ í† í° ìœ„ì¹˜\n",
    "    return h[torch.arange(h.size(0)), last_idx]  # [B, H]\n",
    "\n",
    "# ë°ëª¨ ë°°ì¹˜: ê° íƒœìŠ¤í¬ 1ê°œì”©\n",
    "ids_cls = pack_classification(\"ì˜¤ëŠ˜ ì €ë… ë©”ë‰´ê°€ ê³ ë¯¼ì´ì•¼\")\n",
    "ids_ent = pack_entailment(\"ë¹„ê°€ ë§ì´ ì˜¨ë‹¤\", \"ê¸¸ì´ ë¯¸ë„ëŸ½ë‹¤\")\n",
    "ids_sim_ab, ids_sim_ba = pack_similarity(\"ê³ ì–‘ì´ê°€ ê·€ì—½ë‹¤\", \"ëƒ¥ì´ê°€ ì˜ˆì˜ë‹¤\")\n",
    "ids_mc1 = pack_mc(\"ì§€êµ¬ëŠ” ë¬´ì—‡ì˜ ì£¼ìœ„ë¥¼ ë„ëŠ”ê°€?\", \"íƒœì–‘\")\n",
    "ids_mc2 = pack_mc(\"ì§€êµ¬ëŠ” ë¬´ì—‡ì˜ ì£¼ìœ„ë¥¼ ë„ëŠ”ê°€?\", \"ë‹¬\")\n",
    "\n",
    "input_ids, attn, pos = make_inputs([ids_cls, ids_ent, ids_sim_ab, ids_sim_ba, ids_mc1, ids_mc2])\n",
    "vecs = last_hidden_vec(input_ids, attn, pos)\n",
    "\n",
    "print(\"ì€ë‹‰ ë²¡í„° shape:\", tuple(vecs.shape))         # (ë°°ì¹˜, ì€ë‹‰ì°¨ì›) ì˜ˆ: (6, H)\n",
    "print(\"attention_mask(1í–‰ ì• 32):\", attn[0, :32].tolist())\n",
    "print(\"position_ids(1í–‰ ì• 32):\", pos[0, :32].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4210a-bed3-428d-951b-4f25a250f47e",
   "metadata": {},
   "source": [
    "[ì…ë ¥ ë¸”ë¡ ì¦ë¹™(IDs, Mask, Position, Last hidden)]\n",
    "- íŒ¨ë”©ì„ ì œì™¸í•œ ìœ„ì¹˜ ì¸ë±ì‹±ê³¼ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ í™•ì¸í–ˆìœ¼ë©°, ë§ˆì§€ë§‰ í† í° ì€ë‹‰ ìƒíƒœë¥¼ ì‹œí€€ìŠ¤ í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©í•¨ì„ ì¶œë ¥ìœ¼ë¡œ ì¦ë¹™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e436d3-49d7-40d5-a8cd-aa3ec0b3b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: (6, 256)\n",
      "attention_mask shape: (6, 256)\n",
      "position_ids shape: (6, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids shape:\", tuple(input_ids.shape))\n",
    "print(\"attention_mask shape:\", tuple(attn.shape))\n",
    "print(\"position_ids shape:\", tuple(pos.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a9449-9e59-403b-930d-633cfbd54ee4",
   "metadata": {},
   "source": [
    "4-A. íŒŒë¼ë¯¸í„° ìš”ì•½ + ë¶„ë¥˜ í—¤ë“œ ì¤€ë¹„\n",
    "- ì¶”ê°€ ì•„í‚¤í…ì²˜ëŠ” ì„ í˜• í—¤ë“œë¡œ ì œí•œí–ˆê³ , ë°±ë³¸ì€ ê³ ì •(freeze)í•˜ì—¬ ì „ì´ ì‹œ íŒŒë¼ë¯¸í„° ì¦ê°€ë¥¼ ìµœì†Œí™”\n",
    "- ì„ í˜• í—¤ë“œ(W_y)ë§Œ í•™ìŠµ ëŒ€ìƒìœ¼ë¡œ ë‘ê³ , íŒŒë¼ë¯¸í„° ìš”ì•½ì„ í†µí•´ ì¶”ê°€ íŒŒë¼ë¯¸í„°ê°€ ìµœì†Œì„ì„ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b93952-5038-4cdf-aacd-f0cbcc8c4837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Backbone] total=125,166,336  trainable=125,166,336\n",
      "[Head:cls] params=1,538\n"
     ]
    }
   ],
   "source": [
    "# 4-A) íŒŒë¼ë¯¸í„° ìš”ì•½ + ë¶„ë¥˜ í—¤ë“œ\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "hidden_size = model.transformer.config.n_embd  # KoGPT2 íˆë“ ì°¨ì›\n",
    "\n",
    "# ë¶„ë¥˜ìš© Linear í—¤ë“œ (ì˜ˆ: ì´ì§„)\n",
    "cls_head = nn.Linear(hidden_size, 2).to(device)\n",
    "\n",
    "# ë°±ë³¸ ê³ ì •(ì¶”ê°€ íŒŒë¼ë¯¸í„° ìµœì†Œí™”)\n",
    "freeze_backbone = True\n",
    "if freeze_backbone:\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def print_param_summary(backbone, heads: dict):\n",
    "    total = sum(p.numel() for p in backbone.parameters())\n",
    "    trainable = sum(p.numel() for p in backbone.parameters() if p.requires_grad)\n",
    "    print(f\"[Backbone] total={total:,}  trainable={trainable:,}\")\n",
    "    for name, head in heads.items():\n",
    "        t = sum(p.numel() for p in head.parameters())\n",
    "        print(f\"[Head:{name}] params={t:,}\")\n",
    "\n",
    "print_param_summary(model, {\"cls\": cls_head})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebde005-0474-439c-aefa-98e7eea78645",
   "metadata": {},
   "source": [
    "4-B. ë°ëª¨ìš© DataLoader (ì•„ì£¼ ì‘ì€ ì´ì§„ ë¶„ë¥˜)\n",
    "- ì†Œí˜• ì˜ˆì‹œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì…ë ¥ ë³€í™˜ì´ ì§€ë„í•™ìŠµ ë£¨í‹´ê³¼ ì •í•©ì ìœ¼ë¡œ ì—°ê²°ë¨ì„ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7dc3aba-7983-4db4-87d3-5df46b6395de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, torch.Size([3, 256]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-B) ë°ëª¨ ë°ì´í„°(ì´ì§„ ê°ì„± ë¶„ë¥˜ ì˜ˆì‹œ) â†’ DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "demo_pairs = [\n",
    "    (\"ì´ ì˜í™” ì •ë§ ì¢‹ì•˜ë‹¤\", 1),\n",
    "    (\"ë„ˆë¬´ ì§€ë£¨í•˜ê³  ë³„ë¡œì˜€ë‹¤\", 0),\n",
    "    (\"ì™„ì „ ë§Œì¡±ìŠ¤ëŸ¬ì› ë‹¤\", 1),\n",
    "    (\"ë‘ ë²ˆ ë³´ê³  ì‹¶ì§€ ì•Šë‹¤\", 0),\n",
    "    (\"ê°ë™ì ì´ê³  ìµœê³ ì˜ ì‘í’ˆ\", 1),\n",
    "    (\"ì‹œê°„ì´ ì•„ê¹Œì› ë‹¤\", 0),\n",
    "]\n",
    "\n",
    "class SimpleClsSet(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.X = [pack_classification(t) for t, _ in pairs]\n",
    "        self.y = [y for _, y in pairs]\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        ids = torch.tensor(self.X[i], dtype=torch.long)\n",
    "        att = (ids != tok.pad_token_id).long()\n",
    "        return ids, att, torch.tensor(self.y[i], dtype=torch.long)\n",
    "\n",
    "train_loader_cls = DataLoader(SimpleClsSet(demo_pairs), batch_size=3, shuffle=True)\n",
    "len(train_loader_cls.dataset), next(iter(train_loader_cls))[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98802746-9834-4bc4-936b-2812d5339422",
   "metadata": {},
   "source": [
    "4-C. ë¯¸ë‹ˆ íŒŒì¸íŠœë‹ ë£¨í”„(= model.fit ëŒ€ì²´)\n",
    "- ë§ˆì§€ë§‰ í† í° ì€ë‹‰ë²¡í„°ë¥¼ ì‹œí€€ìŠ¤ í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©í•´ ì„ í˜• í—¤ë“œë¥¼ í•™ìŠµí–ˆê³ , epochë³„ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ í†µí•´ ì •ìƒ í•™ìŠµì„ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de31449-4c64-4859-a079-c5993ee04050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss=0.7312 acc=0.333\n",
      "[epoch 2] loss=0.6915 acc=0.333\n",
      "[epoch 3] loss=0.6679 acc=0.500\n"
     ]
    }
   ],
   "source": [
    "# 4-C) ë¯¸ë‹ˆ íŒŒì¸íŠœë‹: epoch ë¡œê·¸ ì¶œë ¥\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "opt = AdamW(cls_head.parameters(), lr=2e-4)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_last_hidden(input_ids, attention_mask):\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    h = out.hidden_states[-1]                     # [B, L, H]\n",
    "    last_idx = attention_mask.sum(dim=1) - 1\n",
    "    vec = h[torch.arange(h.size(0)), last_idx]    # [B, H]\n",
    "    return vec\n",
    "\n",
    "def train_cls_epoch(dataloader):\n",
    "    cls_head.train()\n",
    "    tot_loss, tot_acc, tot_n = 0.0, 0.0, 0\n",
    "    for input_ids, attn, y in dataloader:\n",
    "        input_ids, attn, y = input_ids.to(device), attn.to(device), y.to(device)\n",
    "        vec = get_last_hidden(input_ids, attn)        # ë°±ë³¸ì€ ê³ ì •, ì€ë‹‰ë§Œ ì¶”ì¶œ\n",
    "        logits = cls_head(vec)                        # [B, 2]\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        acc = (pred == y).float().mean().item()\n",
    "        bs = y.size(0)\n",
    "        tot_loss += loss.item() * bs\n",
    "        tot_acc  += acc * bs\n",
    "        tot_n    += bs\n",
    "    return tot_loss / tot_n, tot_acc / tot_n\n",
    "\n",
    "for ep in range(1, 4):\n",
    "    loss, acc = train_cls_epoch(train_loader_cls)\n",
    "    print(f\"[epoch {ep}] loss={loss:.4f} acc={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f8ff8-bd95-4c0a-8b99-c248078db737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db8d513c-452d-4fc0-9fb3-d32cedeb89e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11153, 587)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4) ë°ì´í„°ì…‹/ì½œë ˆì´í„°\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, frame, tokenizer, max_len=128):\n",
    "        self.q = frame['Q'].tolist()\n",
    "        self.a = frame['A'].tolist()\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.q)\n",
    "    def __getitem__(self, i):\n",
    "        q, a = self.q[i], self.a[i]\n",
    "        text = f\"{bos}{q}{SEP}{a}{eos}\"\n",
    "        enc = self.tok(text, truncation=True, max_length=self.max_len, return_tensors=None)\n",
    "        # ë ˆì´ë¸” ë§ˆìŠ¤í‚¹: SEP ì´ì „ê¹Œì§€ -100\n",
    "        ids = enc['input_ids']\n",
    "        sep_id = self.tok.convert_tokens_to_ids(SEP)\n",
    "        try:\n",
    "            sep_pos = ids.index(sep_id)\n",
    "        except ValueError:\n",
    "            sep_pos = len(ids)//2  # í˜¹ì‹œ ëª» ì°¾ìœ¼ë©´ ëŒ€ì¶© ì ˆë°˜(ëª…ì‹œì  ê°€ì •)\n",
    "        labels = [-100]* (sep_pos+1) + ids[sep_pos+1:]\n",
    "        labels = labels[:self.max_len]\n",
    "        ids = ids[:self.max_len]\n",
    "        attn = [1]*len(ids)\n",
    "        # íŒ¨ë”©\n",
    "        pad_id = self.tok.pad_token_id\n",
    "        while len(ids) < self.max_len:\n",
    "            ids.append(pad_id); attn.append(0); labels.append(-100)\n",
    "        return {\"input_ids\": torch.tensor(ids), \"attention_mask\": torch.tensor(attn), \"labels\": torch.tensor(labels)}\n",
    "\n",
    "train_ds = ChatDataset(train_df, tok)\n",
    "valid_ds = ChatDataset(valid_df, tok)\n",
    "len(train_ds), len(valid_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aa7032-2b79-46e5-9041-309b401db66a",
   "metadata": {},
   "source": [
    "[í•™ìŠµ ë¡œê·¸ ì¶œë ¥]\n",
    "- í•™ìŠµ ë¡œê·¸(ì†ì‹¤, ì •í™•ë„) ìº¡ì²˜ë¥¼ ì²¨ë¶€í•´ ëª¨ë¸ êµ¬ì„±ì˜ íƒ€ë‹¹ì„±ê³¼ í•™ìŠµ ì•ˆì •ì„±ì„ ì¦ëª…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa7c78b3-add1-4f88-ba1b-6a044235b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2091' max='2091' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2091/2091 09:32, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.630800</td>\n",
       "      <td>2.533398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.770400</td>\n",
       "      <td>2.217996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.111100</td>\n",
       "      <td>2.024861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.052600</td>\n",
       "      <td>1.978867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2091, training_loss=1.8797353583052647, metrics={'train_runtime': 572.7733, 'train_samples_per_second': 58.416, 'train_steps_per_second': 3.651, 'total_flos': 2184532033536000.0, 'train_loss': 1.8797353583052647, 'epoch': 2.9978494623655916})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•™ìŠµ\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"kogpt2-chatbot\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=valid_ds)\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dabf1923-5bf5-45c4-bbf7-69517240a6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ë¶„ì¼êº¼ì—ìš”.\n"
     ]
    }
   ],
   "source": [
    "# ê°„ë‹¨ ì¸í¼ëŸ°ìŠ¤ í•¨ìˆ˜\n",
    "\n",
    "@torch.no_grad()\n",
    "def chat(prompt, max_new_tokens=64, top_p=0.9, temperature=0.8):\n",
    "    model.eval()\n",
    "    x = f\"{bos}{prompt}{SEP}\"\n",
    "    inp = tok(x, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        eos_token_id=tok.eos_token_id\n",
    "    )\n",
    "    text = tok.decode(out[0], skip_special_tokens=False)\n",
    "    # SEP ì´í›„ë¶€í„° eos ì „ê¹Œì§€ ì˜ë¼ë‚´ê¸°\n",
    "    try:\n",
    "        ans = text.split(SEP,1)[1]\n",
    "    except:\n",
    "        ans = text\n",
    "    ans = ans.replace(tok.eos_token or \"</s>\", \"\").replace(tok.bos_token or \"<s>\", \"\").strip()\n",
    "    return ans\n",
    "\n",
    "print(chat(\"ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢€ ìš°ìš¸í•œë° ë­ í•˜ë©´ ì¢‹ì„ê¹Œ?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f03e84-5484-45ac-ba52-d1c5d59157e0",
   "metadata": {},
   "source": [
    "[ì¶”ë¡  ë°ëª¨]\n",
    "- ì „ì²˜ë¦¬â†’ë°±ë³¸â†’í—¤ë“œâ†’softmaxì˜ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ì„ ì‹œì—°í•´, í•™ìŠµëœ ëª¨ë¸ì´ ì‹¤ì œ ì…ë ¥ì— ëŒ€í•´ ì¼ê´€ëœ ì˜ˆì¸¡ì„ ì‚°ì¶œí•¨ì„ ë³´ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f11eeae-5b74-47cb-a5f0-12ea3a2fdc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì¥: ì´ ì˜í™” ì§„ì§œ ì¬ë°Œì—ˆë‹¤\n",
      "  ê¸ì • í™•ë¥ : 0.3962\n",
      "  ë¶€ì • í™•ë¥ : 0.6038\n",
      "\n",
      "ë¬¸ì¥: í•˜í’ˆ ë‚˜ì˜¤ëŠ” ì§€ë£¨í•œ ì˜í™”ì˜€ë‹¤\n",
      "  ê¸ì • í™•ë¥ : 0.3311\n",
      "  ë¶€ì • í™•ë¥ : 0.6689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ ë°ëª¨\n",
    "\n",
    "model.eval()\n",
    "cls_head.eval()\n",
    "\n",
    "test_sentences = [\n",
    "    \"ì´ ì˜í™” ì§„ì§œ ì¬ë°Œì—ˆë‹¤\",\n",
    "    \"í•˜í’ˆ ë‚˜ì˜¤ëŠ” ì§€ë£¨í•œ ì˜í™”ì˜€ë‹¤\"\n",
    "]\n",
    "\n",
    "for s in test_sentences:\n",
    "    ids = torch.tensor(pack_classification(s)).unsqueeze(0).to(device)\n",
    "    att = (ids != tok.pad_token_id).long().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=ids, attention_mask=att, output_hidden_states=True)\n",
    "        h = out.hidden_states[-1]\n",
    "        last_idx = att.sum(dim=1) - 1\n",
    "        vec = h[torch.arange(h.size(0)), last_idx]\n",
    "        logits = cls_head(vec)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    print(f\"ë¬¸ì¥: {s}\")\n",
    "    print(f\"  ê¸ì • í™•ë¥ : {probs[0][1].item():.4f}\")\n",
    "    print(f\"  ë¶€ì • í™•ë¥ : {probs[0][0].item():.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9c81aa9-2273-4149-a52c-aafb80d39ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('kogpt2_gpt1_style/best/tokenizer_config.json',\n",
       " 'kogpt2_gpt1_style/best/special_tokens_map.json',\n",
       " 'kogpt2_gpt1_style/best/vocab.json',\n",
       " 'kogpt2_gpt1_style/best/merges.txt',\n",
       " 'kogpt2_gpt1_style/best/added_tokens.json',\n",
       " 'kogpt2_gpt1_style/best/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) ì €ì¥ & ë¡œë“œ\n",
    "\n",
    "trainer.save_model(\"kogpt2_gpt1_style/best\")\n",
    "tok.save_pretrained(\"kogpt2_gpt1_style/best\")\n",
    "# ë¡œë“œ\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"kogpt2-chatbot/best\"); tok = AutoTokenizer.from_pretrained(\"kogpt2-chatbot/best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "231ac109-5773-4d9a-a2c1-731d6ea9eaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë”± ì¢‹ì•„í•  ê±°ì˜ˆìš”.\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"ì´ ì˜í™” ë³„ë¡œì•¼\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48fdc009-179e-48ec-8de8-ca64426f6457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜í™”ë„ ì¢‹ì§€ë§Œ ì°¨ë„ ë§ˆì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"ë‚˜ëŠ” í•˜í’ˆë‚˜ì˜¤ëŠ” ì˜í™”ë¥¼ ì¢‹ì•„í•´\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9cba018-d82d-4af2-89c5-b1e3abadbbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ë„ ì‹«ì–´ìš”.\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"ì‚¬ì‹¤ ë‚˜ëŠ” í•˜í’ˆë‚˜ì˜¤ëŠ” ì˜í™”ë¥¼ ì‹«ì–´í•´\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a90a3-0d37-4761-8dec-1e338a746c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
